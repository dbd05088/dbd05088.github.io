<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Factorizing Perception and Policy for Interactive Instruction Following">
  <meta name="keywords" content="Embodied AI, Modularization, Object-Centric">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Factorizing Perception and Policy for Interactive Instruction Following</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://bhkim94.github.io">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://bhkim94.github.io/projects/CL-ALFRED">
            CL-ALFRED
          </a>
          <a class="navbar-item" href="https://bhkim94.github.io/projects/CAPEAM">
            CAPEAM
          </a>
          <a class="navbar-item" href="https://bhkim94.github.io/projects/MCR-Agent">
            MCR-Agent
          </a>
          <a class="navbar-item" href="https://bhkim94.github.io/projects/ABP">
            ABP
          </a>
          <a class="navbar-item" href="https://bhkim94.github.io/projects/MOCA">
            MOCA
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered is-full-width">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title" style="font-size:45px">
            Factorizing Perception and Policy for Interactive Instruction Following
          </h1>
          <h1 class="title is-4 publication-title">
            ICCV 2021
          </h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://kunalmessi10.github.io/">Kunal Pratap Singh</a><sup>*,2,§</sup>,
            </span>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/suvaansh-bhambri-1784bab7/">Suvaansh Bhambri</a><sup>*,1</sup>,
              </span>
            <span class="author-block">
              <a href="https://bhkim94.github.io">Byeonghwi Kim</a><sup>*,1</sup>,
            </span>
            <span class="author-block">
              <a href="http://roozbehm.info/">Roozbeh Mottaghi</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://ppolon.github.io">Jonghyun Choi</a><sup>1,†</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>GIST, South Korea</span>,
            <span class="author-block"><sup>2</sup>Allen Institute for AI</span>
            <br>
            <span class="author-block"><sup>§</sup>Work done while with GIST</span>
            <span class="author-block"><sup>†</sup>Corresponding author</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Singh_Factorizing_Perception_and_Policy_for_Interactive_Instruction_Following_ICCV_2021_paper.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2012.03208"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/gistvision/moca"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data (Coming Soon)</span>
                  </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src='static/figures/teaser.png' style="margin-left: auto; margin-right: auto; display: block; border-style: none width:80%;max-width:80%">
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">Modular Object-Centric Approach (MOCA)</span>
      </h2>
    </div>
  </div>
</section>



<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Performing simple household tasks based on language directives is very natural to humans, yet it remains an open challenge for AI agents. The ‘interactive instruction following’ task attempts to make progress towards building agents that jointly navigate, interact, and reason in the environment at every step.
              To address the multifaceted problem, we propose a model that factorizes the task into interactive perception and action policy streams with enhanced components and name it as MOCA, a Modular Object-Centric Approach.
              We empirically validate that MOCA outperforms prior arts by significant margins on the ALFRED benchmark with improved generalization.
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-widescreen">
    <h2 class="title is-3">Factorizing Perception and Policy</h2>
    <div class="content has-text-justified">
      <p>
        An interactive instruction following agent performs a sequence of navigational steps and object interactions based on egocentric visual observations it receives from environment.
        These actions and interactions are based on natural language instructions that the agent must follow to accomplish the task.
        We approach this by factorizing the model into two streams, i.e. interactive perception and action policy, and train the entire architecture end-to-end.
      </p>
      <p>
        Action prediction requires global scene-level understanding of the visual observation to abstract it to an action.
On the other hand, for object interaction, the agent needs to focus on both scene-level and object-specific features to achieve precise localisation.
Given the contrasting nature of the two tasks, MOCA has separate streams for action prediction with <span style="font-weight:bold">Action Policy Module (APM)</span> and object localisation with <span style="font-weight:bold">Interactive Perception Module (IPM)</span>.
      </p>
      <div class="columns is-centered has-text-centered">
        <div class="column is-fullwidth">
          <img src='static/figures/overview.png' style="margin-left: auto; margin-right: auto; display: block; border-style: none width:100%;max-width:100%">
        </div>
      </div>
    </div>

    <h3 class="title is-4">Language Guided Dynamic Filters</h3>
    <div class="columns is-centered">
      <div class="column is-two-quarters">
        <div class="content has-text-justified">
          <p>
            Visual grounding helps the agent to exploit the relationships between language and visual features.
            This reduces the agent’s dependence on any particular modality while encountering unseen scenarios.
            It is a common practice to concatenate flattened visual and language features.
            However, it might not perfectly capture the relationship between visual and textual embeddings, leading to poor performance of interactive instruction following agents.
            Dynamic filters are conditioned on language features making them more adaptive towards varying inputs.
            This is in contrast with traditional convolutions which have fixed weights after training and fail to adapt to diverse instructions.
            Hence, we propose to use dynamic filters for the interactive instruction following task.
            Particularly, we use a filter generator network comprising of fully connected layers to generate dynamic filters which attempt to capture various aspects of the language from the attended language features.
          </p>
        </div>
      </div>
    </div>

    <h3 class="title is-4">Object-Centric Localisation</h3>
    <div class="columns is-centered">
      <div class="column is-two-quarters">
        <div class="content has-text-justified">
          <p>
            We bifurcate mask prediction into <em>target class prediction</em> and <em>instance association</em>.
            This bifurcation enables us to leverage the quality of pre-trained segmentation models while also ensuring accurate localisation.
          </p>
          <p>
            <span style="font-weight:bold">Target Class Prediction.</span>
MOCA first predicts the target object class that it intends to interact with at the current time step.
The predicted class is then used to acquire the set of instance masks corresponding to the predicted class from the mask generator.
          </p>
          <p>
            <span style="font-weight:bold">Instance Association.</span>
            A straightforward solution to choose one of the correct mask instances from a pre-trained mask generator is to pick the highest confidence instance.
            However, when it interacts with the same object over an interval, it is more important to remember the object the agent has interacted with, since its appearance might vary drastically during multiple interactions.
            To address all the scenarios, we propose a two-way criterion to select the best instance mask.
            The confidence based criterion chooses the mask with the highest confidence score.
            On the contrary, the association based criterion chooses the mask whose center is closest to the one that the agent has interacted before.
          </p>
        </div>
      </div>
      <div class="column is-two-quarter">
        <img src='static/figures/iat.png' style="margin-left: auto; margin-right: auto; display: block; border-style: none width:100%;max-width:100%">
      </div>
    </div>

    <h3 class="title is-4">Obstruction Evasion</h3>
    <div class="columns is-centered">
      <div class="column is-two-quarters">
        <div class="content has-text-justified">
          <p>
            The agent learns to not encounter any obstacles during training based on the expert ground truth actions.
            However, during inference, there are various situations when the agent gets stranded around immovable objects.
            To address such unanticipated situations, we propose an ‘obstruction evasion’ mechanism in the APM to avoid obstacles at inference time.
            While navigating in the environment, at every time step, the agent computes the distance between visual features at the current time step and the previous time step with a tolerance hyper-parameter.
            If the distance is within the tolerence, we regard the current state as obstruction.
            To evade the obstruction, we remove the action that causes the obstruction from the action space and instead take the second-best action (i.e., the action with the second-highest confidence score).
          </p>
        </div>
      </div>
      <div class="column is-two-quarter">
        <img src='static/figures/oe.png' style="margin-left: auto; margin-right: auto; display: block; border-style: none width:100%;max-width:100%">
      </div>
    </div>

</section>


<section class="hero is-light">
  <div class="hero-body">
    <div class="container is-max-widescreen">
      <h2 class="title is-3">Results</h2>
      <div class="content has-text-justified">
        <p>
          We employ <span style="color: #305fac; font-weight:bold"><a href="https://askforalfred.com/">ALFRED</a></span> to evaluate our method.
          There are three splits of environments in ALFRED: ‘train’, ‘validation’, and ‘test’.
          The validation and test environments are further divided into two folds, seen and unseen, to assess the generalization capacity.
          The primary metric is the success rate, denoted by ‘SR,’ which measures the percentage of completed tasks.
          Another metric is the goal-condition success rate, denoted by ‘GC,’ which measures the percentage of satisfied goal conditions.
          Finally, path-length-weighted (PLW) scores penalize SR and GC by the length of the actions that the agent takes.
        </p>
        <p>
          As shown in the figure, MOCA shows significant improvement over the prior arts on all metrics. The higher success rate in the unseen scenes indicates its ability to generalize in novel environments.
          We achieve an improvement of 14.42% and 3.20% in Seen and Unseen Task SR over Nguyen et al. that won ALFRED challenge in ECCV 2020.
        </p>
        <p>
          For more details, please check out the <span style="color: #305fac; font-weight:bold"><a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Singh_Factorizing_Perception_and_Policy_for_Interactive_Instruction_Following_ICCV_2021_paper.pdf">paper</a></span>.
        </p>
        <br>
        <div class="columns is-centered has-text-centered">
          <div class="columns is-centered has-text-centered">
            <div class="column is-full-width">
              <img src="static/tables/comparison_with_sota.png">
              <h5>Comparison with State of the Art</h5>
            </div>
          </div>
        </div>

        <div class="columns is-centered has-text-centered">
          <div class="column is-two-quarter">
            <img src="static/figures/qualitative_nofpp.png">
            <h5> MOCA w/o factorization</h5>
          </div>

          <div class="column is-two-quarter">
            <img src="static/figures/qualitative_fpp.png">
            <h5>MOCA w/ factorization</h5>
          </div>
        </div>

        <div class="columns is-centered has-text-centered">
          <div class="column is-one-quarter">
            <img src="static/figures/qualitative_noocl.png">
            <h5> MOCA w/o OCL</h5>
          </div>
          <div class="column is-one-quarter">
            <img src="static/figures/qualitative_ocl.png">
            <h5> MOCA w/ OCL</h5>
          </div>
          <div class="column is-one-quarter">
            <img src="static/figures/qualitative_noocl2.png">
            <h5> MOCA w/o OCL</h5>
          </div>
          <div class="column is-one-quarter">
            <img src="static/figures/qualitative_ocl2.png">
            <h5> MOCA w/ OCL</h5>
          </div>
        </div>

      </div>
    </div>
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{singh2021factorizing,
  author    = {Singh, Kunal Pratap and Bhambri, Suvaansh and Kim, Byeonghwi and Mottaghi, Roozbeh and Choi, Jonghyun},
  title     = {Factorizing Perception and Policy for Interactive Instruction Following},
  booktitle = {ICCV},
  year      = {2021},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          The website template is from <a href="https://github.com/nerfies/nerfies.github.io">here</a>.
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
